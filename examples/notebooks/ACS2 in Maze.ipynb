{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACS2 in Maze Experiments\n",
    "The following notebooks provides reproducible playground for experimenting how ACS2 agent behaves inside Maze environments.\n",
    "\n",
    "TODO:\n",
    "- check if classifiers with $q < 0.1$ are deleted,\n",
    "\n",
    "#### Resources\n",
    "- Nice [presentation](http://www.psychologie.uni-wuerzburg.de/stolzmann/ACS-tutorial/ACS-tutorial.pdf),\n",
    "\n",
    "### History of ACS\n",
    "First ALCS ([ACS](http://www.psychologie.uni-wuerzburg.de/stolzmann/gp-98.ps.gz)) was developed by Stolzmanm in 1997 (with puts an additional anticipatory part in each classifier). Another ALCS with an explicit anticipatory part [YACS](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.26.1149), has been published in Gerard and Siguad (2001). Tomlison and Bull (2000) published a cooperate learning classifier system ([CXCS](http://dl.acm.org/citation.cfm?id=689033)) in which cooperations between rules allow antitipatory proceses.\n",
    "\n",
    "\n",
    "ACS2 (derived from Stolzmann's work) is intendet to create a solution that is *complete*, *accurate* and *maximally general*. Major differences between ACS and ACS2:\n",
    "- ACS2 evolves explicit rules for situation-action tuples in which no changes occurs (a *pass-through-symbol* in E part **requires** a change in value),\n",
    "- ACS2's ALP (specialization pressure) and GA (generalization pressure) processes are improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge representation\n",
    "Knowledge in an ACS2 is represented by a population of classifiers. Each classifier represents a *condition-action-effect* that anticipates the model *state* resulting from the execution of the *action* given the specified *conditions*.\n",
    "\n",
    "> A classifier in ACS2 always specifes a complete resulting state.\n",
    "\n",
    "It consists of the following main components:\n",
    "- *condition part (C)* - specifies the set of situations in which a classifier is applicable,\n",
    "- *action part (A)* - proposes an available action,\n",
    "- *effect part (E)* - anticipates the effects of the proposed action in the specific conditions,\n",
    "- *quality (q)* - measures the accuracy of the anticipated results, $q \\in [0,1]$,\n",
    "- *reward prediction (r)* - estimates the reward encountered after the execution of action A in condition C, $r \\in \\!R$\n",
    "- *intermediate reward prediction (ir)* - estimates the *direct* reinforcement encountered after execution of action A in condition C, $ir \\in \\!R$\n",
    "\n",
    "The *condition* and *effect* part consist of the values perceived from the environment and `#` symbols (i.e. $C,E \\in \\{ l_1, l_2, \\dots, l_m, \\# \\}^L$. \n",
    "\n",
    "> A `#`-symbol in the:\n",
    "- condition part is called *\"don't care\"* and denotes that the classifier matches any value in this attribute,\n",
    "- effect part is called *\"pass-through\"* specifies that the classifier anticipates that the value of this attribute will not change after the execution of the specified action\n",
    "\n",
    "> **Non pass-through symbols in E anticipate the change of the particular attribute to the specified value** (in contrast to ACS in which a non pass-through symbol did not require a change in value).\n",
    "\n",
    "Additionally each classifier compromises:\n",
    "- *Mark (M)* - records the values of each attribute of all situations in which the classifer did not anticipate correctly sometimes,\n",
    "- *GA timestamp ($t_{ga}$)* - timestamp when GA was last applied,\n",
    "- *ALP timestamp ($t_{alp}$)* - timestamp when ALP was last applied,\n",
    "- *application average (aav)* - estimates the frequency a classifier is updated (i.e. part of an action set),\n",
    "- *experience counter (exp)* - counts the number of applications,\n",
    "- *numerosity (num)* - denotes the number of micro-classifers this macroclassifier represents (one classifier may represent many identical micro-classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent interaction\n",
    "ACS2 interacts *autonomously* with an environment.\n",
    "<img src=\"temp/agent_environment_interaction.png\" alt=\"agent_env_interaction\" style=\"width: 400px;\"/>\n",
    "\n",
    "In a *behavioral act* at a certain time $t$, the agent perceives a situation $\\sigma(t) = \\{ l_1, l_2, \\dots, l_m \\}^L$, where: \n",
    "\n",
    "- $m$ denotes the number of possible values of each environmental attribute (or feature), \n",
    "- $l_1, l_2, \\dots, l_n$ denote the different possible values for each attribute,\n",
    "- $L$ denotes the string length.\n",
    "\n",
    "> Note that each attribute can only take **discrete** values.\n",
    "\n",
    "The system can act upon the environment with an action $\\alpha(t) = \\{ \\alpha_1, \\alpha_2, \\dots, \\alpha_n \\}$, where:\n",
    "\n",
    "- $n$ specifies the number of different possible actions in in the environment,\n",
    "- $\\alpha_1, \\alpha_2, \\dots, \\alpha_n$ denote the different possible actions\n",
    "\n",
    "After the execution of an action, the environment provides a scalar reinforcement value $\\rho(t) \\in \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environmental Model\n",
    "By interacting with the environemnt the ACS2 learns about it's structure. Usually the agent starts without any prior knowledge. Initially new classifiers are mainly generated by a *covering* mechanizm in ALP. Later the ALP generates specialized clasifiers while the GG tries to introduce some genetic generalization.\n",
    "\n",
    "Figure below presents the interaction with greater details.\n",
    "1. After the **perception of the current situation** $\\sigma(t)$, ACS2 forms a **match set** `[M]` comprising all classifiers in the population `[P]` whose conditions are satisfied in $\\sigma(t)$,\n",
    "2. ACS2 **chooses an action** $\\alpha(t)$ according to some strategy (see below),\n",
    "3. With respect to the chosen action, an **action set** `[A]` is generated that consist of all classifiers in `[M]` that specify the chosen action $\\alpha(t)$,\n",
    "4. After the execution of $\\alpha(t)$ **classifier parameters are updated** by ALP and RL. New classifiers might be added or deleted due to the ALP and GG (see below).\n",
    "\n",
    "\n",
    "<img src=\"temp/ACS2.png\" alt=\"Environmental model\" style=\"width: 600px;\"/>\n",
    "\n",
    "##### Action selection Strategies\n",
    "Possible strategies used for choosing which action to execute next:\n",
    "\n",
    "- $\\epsilon$-greedy (Sutton and Barto, 1998, often used in reinforcement learning),\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anticipatory Learning Process (ALP)\n",
    "The ALP was originally derived from the cognitive theory of anticipatory behavioral control (Hoffman, 1993). The process results in the evaluation and specialization of the anticipatory model in ACS2.\n",
    "\n",
    "> ALP first update classifier parameters. Next an offspring is generated and inaccuate classifiers are removed.\n",
    "\n",
    "##### Parameter updates\n",
    "The following parameters are updated in the **following order** - quality ($q$), mark ($M$), application average ($aav$), ALP timestamp ($t_{alp}$) and the experience counter ($exp$).\n",
    "\n",
    "###### Quality\n",
    "The quality $q$ is updated according to the classifiers anticipation. If the classifier predicted correctly, the quality is increased using the following formula:\n",
    "$$q \\leftarrow q + \\beta (1- q)$$\n",
    "\n",
    "Otherwise it is decreased:\n",
    "$$q \\leftarrow q - \\beta q$$\n",
    "\n",
    "> In the equation, $\\beta \\in [0,1]$ denotes the **learning rate** of ACS2. The smaller the learning rate, the more passive ACS2 updates its values and the less are the values biased towards recent environmental interactions. On the other hand, the larger the learning rate, the faster the parameters adapt to changes in the environment but also the more noisy are the parameters.\n",
    "\n",
    "###### Mark\n",
    "Situation $\\sigma(t) = (\\sigma_1, \\dots, \\sigma_L)$ is added to the mark $M = (m_i, \\dots, m_L)$ if the classifier did not anticipate correctly. In this case $\\forall_i m_i = m_i \\cup \\{ \\sigma_i\\} $.\n",
    "\n",
    "###### Application average\n",
    "Parameter is updated using the *\"moyenne adaptive modifiee\"* technique as introduced in Venturini (1994).\n",
    "\n",
    "$$\n",
    "cl.aav \\leftarrow \\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  \\frac{cl.aav(cl.exp -1) + (t - cl.t_{alp})}{cl.exp} & \\text{if}\\ cl.exp < \\frac{1}{\\beta}\\\\\n",
    "                  cl.aav + \\beta (t - cl.t_{alp}) & \\text{otherwise}\\\\\n",
    "                \\end{array}\n",
    "              \\right.\n",
    "$$\n",
    "\n",
    "> The technique assures the fast adaptation of $aav$ once the classifier is introduced and later assures a continues update according to the overall learning rate $\\beta$. Note, this technique also introduces a possible high factor of noise in the young classifier. Thus, the technique is not applied in the quality updates.\n",
    "\n",
    "###### ALP timestamp\n",
    "The ALP time stamp is set to the current time t recording the last parameter update in the ALP.\n",
    "$$cl.t_{alp} \\leftarrow t$$\n",
    "\n",
    "###### Experience\n",
    "Increment experience by 1\n",
    "\n",
    "$$cl.exp \\leftarrow cl.exp + 1$$\n",
    "\n",
    "##### Classifier generation and deletion\n",
    "The ALP generates specialized offspring and/or deletes *inaccurate* classifiers.\n",
    "\n",
    "> Inaccurate classifiers are classifiers whose quality is lower than the inaccuracy threshold $\\theta_i$. When the quality of a classifiers falls below $\\theta_i$ after an update, it is deleted.\n",
    "\n",
    "More specialized classifiers are generated in two ways.\n",
    "\n",
    "1. An **expected case**, in which a classifier anticipated the correct outcome, a classifier *might* be generated if the mark $M$ differs from the situation $\\sigma(t)$ in some attributes, i.e. $\\exists_{i,j} l_j \\in m_i \\wedge l_j \\neq \\sigma_i$. Since the mark specifies the characteristics of situations in which a classifier did not work correctly, a **difference indicates that the specific position might be important to distinguish the correct and wrong outcome case**. Thus, the ALP generates an offspring whose conditions are further specialized. If there are unique differences in the mark compared to the current situation, i.e. $\\exists_i \\sigma_i \\notin m_i$, then one of the unique difference is specialized in the offspring. However, if there are only positions that differ but $\\sigma_i$ is always part of $m_i$, i.e. $\\forall_i \\sigma_i \\in m_i$, then all differing positions are specialized.\n",
    "> The number of specialized positions in the conditions that are not specialized in the effects is limited to $u_{max}$.\n",
    "\n",
    "2. In an **unexpected case**, a classifier did not anticipate the correct outcome. In this case, an offspring classifier is generated, if the effect part of the classifier can be further specialized (by changing pass-through symbols to specific values) to specify the perceived outcome correctly. All positions in condition and effect part are specialized that change from $\\sigma(t)$ to $\\sigma(t+1)$.\n",
    "\n",
    "> In both reproduction cases:\n",
    "- the Mark $M$ of the offspring is emptied - $cl.M \\leftarrow \\emptyset$,\n",
    "- the experience counter $exp$ is set to zero - $cl.exp \\leftarrow 0$, \n",
    "- ALP and GA time stamp are set to the current time t - $cl.t_{alp}, cl.t_{ga} \\leftarrow t$,\n",
    "- the numerosity is set to one - $cl.num \\leftarrow 0$,\n",
    "- all other parameters are inherited from the parents.\n",
    "\n",
    "> If the generated offspring already exists in the population `[P]`, the offspring is discarded and the quality q of the old classifier is increased applying equation 1.\n",
    "    \n",
    "    \n",
    "A classifier is also generated if there was no other classifier in the actual action set `[A]` that anticipated the effect correctly. In this case, a **covering classifier** is generated that is specialized in all attributes in condition and effect part that changed from $\\sigma(t)$ to $\\sigma(t+1)$.\n",
    "> The covering method was not applied in ACS since in ACS a completely general classifiers was always present for each action.\n",
    "\n",
    "The attributes of the Mark $M$ of the covering classifier are initially empty. Quality $q$ is set to 0.5 as well as the reward prediction $r$, while the immediate reward prediction $ir$ as well as the application average $avv$ are set to 0. The time stamps are set to the current time $t$.\n",
    "$$\n",
    "cl.M \\leftarrow \\emptyset \\\\\n",
    "cl.q \\leftarrow 0.5 \\\\\n",
    "cl.r \\leftarrow 0.5 \\\\\n",
    "cl.ir \\leftarrow 0 \\\\\n",
    "cl.aav \\leftarrow 0 \\\\\n",
    "cl.t_{alp} \\leftarrow t \\\\\n",
    "cl.t_{ga} \\leftarrow t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genetic Generalization (GG)\n",
    "While the ALP specializes classifiers in a quite competent way, over-specializations can occur sometimes as studied in (Butz, 2001). Since the over-specialization cases can be caused by various circumstances, a *genetic generalization* (GG) mechanism was applied that, interacting with the ALP, results in the evolution of a complete, accurate, and maximally general model.\n",
    "\n",
    "The mechanism starts after applying ALP module, and looks as follow:\n",
    "\n",
    "1. **Determine if GG should be applied**. GG is applied if the average time since last GG application in the current action set `[A]` is larger than the threshold $\\theta_{ga}$.\n",
    "$$\n",
    "t - \\frac{\\sum_{cl \\in [A]} cl.t_{ga} cl.num}{\\sum_{cl \\in [A]} cl.num} > \\theta_{ga}\n",
    "$$\n",
    "\n",
    "2. If the mechanism is applied update the application time for all classifiers  $\\forall_{cl \\in [A]} cl.t_{ga} \\leftarrow t$,\n",
    "\n",
    "3. Select two classifiers using *\"roulette-wheel selection\"*, with respect to their qualities $q$.\n",
    "4. Reproduce two classifiers, by removing marks and halving the qualities.\n",
    "$$\n",
    "cl_1.M \\leftarrow \\emptyset \\\\\n",
    "cl_2.M \\leftarrow \\emptyset \\\\\n",
    "cl_1.q \\leftarrow \\frac{cl_1.q}{2} \\\\\n",
    "cl_2.q \\leftarrow \\frac{cl_2.q}{2}\n",
    "$$\n",
    "5. Mutate classifiers, by applying a *\"generalizing mutation\"*.\n",
    ">Generalizing mutation is only mutating **specified** attributes in the condition part C back to don’t care `#` symbols. A specialized attribute is generalized with a probability $\\mu$. Moreover, conditions of the offspring are crossed applying **two-point crossover** with a probability of $\\chi$. In the case of a crossover application, quality, reward prediction, and immediate reward prediction are averaged over the offspring.\n",
    "\n",
    "6. Insert new offspring classifiers into `[P]` and `[A]`\n",
    "> If a generated offspring already exists in the population, the offspring classifier is discarded and if the existing classifier is not marked its numerosity is increased by one.\n",
    "$$\n",
    "cl.M \\leftarrow \\emptyset \\\\\n",
    "cl.num \\leftarrow cl.num + 1\n",
    "$$\n",
    "\n",
    "The GG mechanism also applies a **deletion procedure** inside the action sets. If an action set `[A]` exceeds the action set size threshold $\\theta_{as}$, excess classifiers are deleted in `[A]`. The procedure applies a *\"tournament selection\"* process in which the classifier with the significant lowest quality, or the classifier with the highest specificity is deleted. Thus, deletion causes the extinction of low-quality as well as over-specialized classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinforcement Learning (RL)\n",
    "About RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and configuration\n",
    "Load project files from upper directory (nasty solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/khozzy/Projects/pyalcs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `pyalcs` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Enable automatic module reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from alcs.agent.acs2 import ACS2\n",
    "from alcs.environment.maze import Maze\n",
    "\n",
    "# Metrics\n",
    "from alcs.helpers.metrics import \\\n",
    "    ActualStep,\\\n",
    "    ClassifierPopulationSize,\\\n",
    "    AveragedFitnessScore,\\\n",
    "    SuccessfulTrial,\\\n",
    "    AveragedConditionSpecificity,\\\n",
    "    AchievedKnowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "We are going to test ACS2 **agent** inside **maze** environment. The agent will have 2 tasks:\n",
    "1. learn the shortest path to the goal state,\n",
    "2. learn the internal model of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maze\n",
    "Currently there are 4 mazes implemented:\n",
    "- `mazes/MazeF1.maze` (deterministic),\n",
    "- `mazes/MazeF2.maze` (deterministic),\n",
    "- `mazes/MazeF3.maze` (deterministic),\n",
    "- `mazes/MazeF4.maze` (non-deterministic)\n",
    "\n",
    "Maze `MazeF4` introduces also the *aliasing* problem - the are two states that looks exactly the same for an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Location of the maze file\n",
    "MAZE_LOCATION = 'mazes/MazeF2.maze'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAGJCAYAAAAT91/9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1070184a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = Maze(MAZE_LOCATION)\n",
    "\n",
    "fig = plt.figure(figsize=(env.max_x, env.max_y))\n",
    "ax = fig.add_subplot(111, aspect='equal')\n",
    "\n",
    "# Render maze as image\n",
    "plt.imshow(env.matrix, interpolation='none', cmap='Set3', aspect='auto',\n",
    "           extent=[0, env.max_x, env.max_y, 0])\n",
    "\n",
    "# Add labels to each cell\n",
    "for x in range(0, env.max_x):\n",
    "    for y in range(0, env.max_y):\n",
    "        plt.text(x+0.4, y+0.5, env.matrix[y][x])\n",
    "\n",
    "ax.set_title(MAZE_LOCATION)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "plt.xlim(0, env.max_x)\n",
    "plt.ylim(env.max_y, 0)\n",
    "\n",
    "plt.xticks(range(0, env.max_x))\n",
    "plt.yticks(range(0, env.max_y))\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Legend / reward map**:\n",
    "- `0` is a $wall$ (cannot cross),\n",
    "- `1` is a $path$ (reward $\\rho = 0$),\n",
    "- `9` is a $price$ (reward $\\rho = 1000$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "Agent interacts autonomously with the environment.\n",
    "\n",
    "In this notebook it moves in maze for the certain number of steps.\n",
    "\n",
    "> **Trial** is a successful run of agent when he manages to find the reward.\n",
    "\n",
    "> After learning it should take him less and less steps to reach the goal.\n",
    "\n",
    "At the beginning of each trial an agent is randomly placed inside the maze. He can perceive 4 values (neighbourhood cells - N, E, S, W) and move in 4 directions (N, E, S, W)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the simulation\n",
    "The whole simulation will run a certain number of times for certain number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of experiments\n",
    "EXPERIMENTS = 20\n",
    "\n",
    "# Number of agent steps for each experiment\n",
    "STEPS = 3000 # 10 000\n",
    "\n",
    "# Maximum number of steps in single trial (before resetting an animat)\n",
    "# Set to None for no limits\n",
    "MAX_STEPS_IN_TRIAL = 50\n",
    "\n",
    "# Amount of processes pool\n",
    "PROCESSES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's declare a function performing a single experiment.\n",
    "\n",
    "We are:\n",
    "1. initializing an agent,\n",
    "2. instructing him to collect specified measurements,\n",
    "3. initializing the environment,\n",
    "4. running the simulation,\n",
    "5. returning obtained classifiers and metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def perform_experiment(experiment):\n",
    "    # Initialize the agent\n",
    "    agent = ACS2()\n",
    "    agent.add_metrics_handlers([\n",
    "        ActualStep('time'),\n",
    "        SuccessfulTrial('found_reward'),\n",
    "        ClassifierPopulationSize('total_classifiers'),\n",
    "        AveragedFitnessScore('average_fitness'),\n",
    "        AveragedConditionSpecificity('average_specificity'),\n",
    "        AchievedKnowledge('achieved_knowledge')\n",
    "    ])\n",
    "\n",
    "    # Initialize the environment\n",
    "    env = Maze(MAZE_LOCATION)\n",
    "\n",
    "    # Evaluate algorithm\n",
    "    classifiers, metrics = agent.evaluate(env, STEPS, MAX_STEPS_IN_TRIAL)\n",
    "\n",
    "    # Add information about the experiment into metrics\n",
    "    metrics['experiment_id'] = [experiment] * len(metrics['time'])\n",
    "\n",
    "    return classifiers, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then firing many jobs in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_classifiers = []\n",
    "all_metrics = pd.DataFrame()\n",
    "\n",
    "with ProcessPoolExecutor(PROCESSES) as executor:\n",
    "    futures = []\n",
    "\n",
    "    for i in range(EXPERIMENTS):\n",
    "        future = executor.submit(perform_experiment, i)\n",
    "        futures.append(future)\n",
    "\n",
    "    for idx, el in enumerate(as_completed(futures)):\n",
    "        classifiers, metrics = el.result()\n",
    "\n",
    "        all_classifiers.append(classifiers)\n",
    "        all_metrics = all_metrics.append(pd.DataFrame(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect how the classifier population looks. Internal model of the environment is specified only by reliable classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take classifiers from the first experiment\n",
    "classifiers = all_classifiers[0]\n",
    "\n",
    "# Select only 'reliable' ones\n",
    "reliable = [c for c in classifiers if c.q > 0.9]\n",
    "\n",
    "print(\"Population classifiers: {}\".format(len(classifiers)))\n",
    "print(\"Reliable classifiers: {}\\n\".format(len(reliable)))\n",
    "\n",
    "for cls in reliable:\n",
    "    print(cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\">**TODO**: There are duplicate classifiers!!!! Maybe subsumption does not works. Why low quality classifiers are not removed?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metrics\n",
    "Let's have a look how the performance metrics look like by taking 5 random samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_metrics.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of steps to food\n",
    "Measures if the classifier actually *learns* something along the time.\n",
    "\n",
    "> **Number of steps to food** - How many steps were needed in **each trial** to found the reward.\n",
    "\n",
    "We need to have a little helper function for operating on obtained measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_avg_number_of_steps_to_food(performance):\n",
    "    trials_by_experiment = pd.DataFrame(performance\n",
    "                                        .groupby('experiment_id')\n",
    "                                        .apply(lambda x: x['found_reward'].cumsum())\n",
    "                                        .stack())\n",
    "    trials_by_experiment.reset_index(inplace=True)\n",
    "    trials_by_experiment.rename(columns = {'found_reward': 'time', 0: 'trial'},\n",
    "                                inplace=True)\n",
    "\n",
    "    # Experiment_id, trial, steps\n",
    "    steps_per_trial_in_experiment = pd.DataFrame(trials_by_experiment\n",
    "                                                 .groupby(['experiment_id', 'trial'])\n",
    "                                                 .size())\n",
    "    steps_per_trial_in_experiment.reset_index(inplace=True)\n",
    "    steps_per_trial_in_experiment.rename(columns = {0: 'steps'}, inplace=True)\n",
    "\n",
    "    # Trial, averaged steps\n",
    "    averaged_steps_per_trial = pd.DataFrame(steps_per_trial_in_experiment.groupby('trial')['steps'].mean())\n",
    "    averaged_steps_per_trial.rename(columns = {'steps': 'averaged_steps'}, inplace=True)\n",
    "    averaged_steps_per_trial.reset_index(inplace=True)\n",
    "    \n",
    "    return averaged_steps_per_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "steps_to_food = get_avg_number_of_steps_to_food(all_metrics)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8), dpi=100)\n",
    "\n",
    "plt.bar(steps_to_food['trial'], steps_to_food['averaged_steps'])\n",
    "plt.title('Averaged number of steps to food')\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('Steps')\n",
    "\n",
    "# plt.yscale('log')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Achieved knowledge\n",
    "Measures if an agent is *learning* the environment.\n",
    "\n",
    "> **Achieved knowledge** - test whether or not there is a **reliable classifier** for each transition $(\\sigma_{1}, \\alpha, \\sigma_{2})$ with $\\sigma_{1} \\neq \\sigma_{2}$ that can be applied in $\\sigma_{1}$ and anticipates $\\sigma_{2}$ correctly.\n",
    "\n",
    "This metric is calculated after each time-step done by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_avg_achieved_knowledge_per_step(performance):\n",
    "    knowledge_per_step = pd.DataFrame(performance.groupby('time')['achieved_knowledge'].mean())\n",
    "    knowledge_per_step.reset_index(inplace=True)\n",
    "    return knowledge_per_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knowledge_per_step = get_avg_achieved_knowledge_per_step(all_metrics)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8), dpi=100)\n",
    "\n",
    "plt.plot(knowledge_per_step['time'], knowledge_per_step['achieved_knowledge'], 'r')\n",
    "plt.title('Averaged achieved knowledge')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Achieved knowledge')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specificity measure\n",
    "Measure the *generalization* in classifier population.\n",
    "\n",
    "> **Specificity measure** - Equal to the **sum** of all specialized attributed in condition part of all classifiers in population **divided** by the number of classifiers **times** the length of  the perception $\\sigma$.\n",
    "\n",
    "You can read more about this metric in \"*Introducing a Genetic Generalization Pressure to the\n",
    "Anticipatory Classfierier System - Part2: Performance Analysis*\" by Martin V. Butz.\n",
    "\n",
    "Plot should change after modifing the agent's *mutation* and *cross over* values. The author claims that the best generalization was reached for mutation rate $\\mu = 0.4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_avg_specificity_per_step(performance):\n",
    "    spec_per_step = pd.DataFrame(performance.groupby('time')['average_specificity'].mean())\n",
    "    spec_per_step.reset_index(inplace=True)\n",
    "    return spec_per_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spec_per_step = get_avg_specificity_per_step(all_metrics)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8), dpi=100)\n",
    "\n",
    "plt.plot(spec_per_step['time'], spec_per_step['average_specificity'], 'r', linewidth=2)\n",
    "plt.title('Averaged specificity of condition parts for classifiers')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Specificity')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the value stabilizes near 0.45 which means most elements of condition part is wildcards.\n",
    "\n",
    "We can execute another experiment, but this time also the mutation ratio will change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following image presents how the alternative plot can look like:\n",
    "![title](temp/generalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare variables for the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an array containing possible values of mutation rate\n",
    "mutation_rates = np.arange(0, 1, 0.2)\n",
    "\n",
    "# Number of experiments\n",
    "EXPERIMENTS = 10\n",
    "\n",
    "# Number of steps in each experiment\n",
    "STEPS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "performance = pd.DataFrame()\n",
    "\n",
    "for experiment in range(EXPERIMENTS):\n",
    "    print(\"Experiment: [{}]\".format(experiment))\n",
    "    \n",
    "    for mutation_rate in mutation_rates:\n",
    "        print(\"\\tMutation rate: {}\".format(mutation_rate))\n",
    "        \n",
    "        # Re-initialize the environment\n",
    "        env = Maze(MAZE_LOCATION)\n",
    "\n",
    "        # Initialize the agent with metric handler\n",
    "        agent = ACS2(mu=mutation_rate)\n",
    "        agent.add_metrics_handlers([\n",
    "            ActualStep('time'),\n",
    "            AveragedConditionSpecificity('average_specificity'),\n",
    "        ])\n",
    "        \n",
    "        # Evaluate algorithm\n",
    "        #classifiers, metrics = agent.evaluate(env, STEPS)\n",
    "\n",
    "        # Add information about the experiment into metrics\n",
    "        metrics['experiment_id'] = [experiment] * len(metrics['time'])\n",
    "        metrics['mutation_rate'] = [mutation_rate] * len(metrics['time'])\n",
    "\n",
    "        # Append performance metrics\n",
    "        performance = performance.append(pd.DataFrame(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the condition specificity we will take it's value obtained after last step in each experiment and average it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_avg_specificity_by_mutation_rate(performance):\n",
    "    last_as_by_experiment = pd.DataFrame(performance\n",
    "                                         .groupby(['experiment_id','mutation_rate'])['average_specificity']\n",
    "                                         .last())\n",
    "    last_as_by_experiment.reset_index(inplace=True)\n",
    "    \n",
    "    avg_as_by_mutation_rate = pd.DataFrame(last_as_by_experiment\n",
    "                                           .groupby(['mutation_rate'])['average_specificity']\n",
    "                                           .mean())\n",
    "    avg_as_by_mutation_rate.reset_index(inplace=True)\n",
    "    \n",
    "    return avg_as_by_mutation_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the results look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_as_by_mutation_rate = get_avg_specificity_by_mutation_rate(performance)\n",
    "avg_as_by_mutation_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 8), dpi=100)\n",
    "\n",
    "plt.plot(avg_as_by_mutation_rate['mutation_rate'],\n",
    "         avg_as_by_mutation_rate['average_specificity'], 'r', linewidth=2)\n",
    "plt.title('Averaged specificity ratio')\n",
    "plt.xlabel('Mutation rate')\n",
    "plt.ylabel('Specificity')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
