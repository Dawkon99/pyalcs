{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "#logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACS2 in *FrozenLake*\n",
    "\n",
    "> The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "# Reset the state\n",
    "state = env.reset()\n",
    "\n",
    "# Render the environment\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each state might get following possible values: `{S, F, H, G}` which, refers to\n",
    "```\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "```\n",
    "\n",
    "In case of interacting with environment agent cant perform 4 action which map as follow:\n",
    "- 0 - left\n",
    "- 1 - down\n",
    "- 2 - right\n",
    "- 3 - up\n",
    "\n",
    "> FrozenLake-v0 defines \"solving\" as getting average reward of 0.78 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Agent\n",
    "Unfortunatelly the only information back from the environment is the current agent position. Therefore our agent task will be to predicit where it will land after executing each action.\n",
    "\n",
    "To do so we will represent state as a one-hot encoded vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyALCS code from local path\n",
    "import sys\n",
    "sys.path.append('/Users/khozzy/Projects/pyalcs')\n",
    "\n",
    "from alcs import ACS2, ACS2Configuration\n",
    "\n",
    "# Enable automatic module reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set some constants\n",
    "CLASSIFIER_LENGTH = 16\n",
    "POSSIBLE_ACTIONS = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(state):\n",
    "    vec = ['0' for i in range(CLASSIFIER_LENGTH)]\n",
    "    vec[state] = 'X'\n",
    "    return ''.join(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X` corresponds to current agent position. State 4 is encoded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0000X00000000000'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encode(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need a function for evaluating if agent finished succesfuly a trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume if the final state was with number 15 that the algorithm found the reward. Otherwise not\n",
    "def collect_env_metrics(env):\n",
    "    state = None\n",
    "    \n",
    "    if type(env) is gym.wrappers.monitoring.Monitor:\n",
    "        state = env.env.env.s\n",
    "    else:\n",
    "        state = env.env.s\n",
    "        \n",
    "    return {'found_reward': state == 15}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now we are ready to configure the ACS2 agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACS2Configuration:\n",
      "\t- Classifier length: [16]\n",
      "\t- Number of possible actions: [4]\n",
      "\t- Classifier wildcard: [#]\n",
      "\t- Perception mapper function: [<function one_hot_encode at 0x10ed558c8>]\n",
      "\t- Action mapping dict: [None]\n",
      "\t- Environment metrics function: [<function collect_env_metrics at 0x10ed55840>]\n",
      "\t- Performance calculation function: [None] \n",
      "\t- Do GA: [False]\n",
      "\t- Do subsumption: [True]\n",
      "\t- Beta: [0.05]\n",
      "\t- ...\n",
      "\t- Epsilon: [0.7]\n",
      "\t- U_max: [100000]\n"
     ]
    }
   ],
   "source": [
    "cfg = ACS2Configuration(\n",
    "    classifier_length=CLASSIFIER_LENGTH,\n",
    "    number_of_possible_actions=POSSIBLE_ACTIONS,\n",
    "    perception_mapper_fcn=one_hot_encode,\n",
    "    environment_metrics_fcn=collect_env_metrics,\n",
    "    theta_i=0.3,\n",
    "    epsilon=0.7)\n",
    "\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build agent using defined configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ACS2(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn some behaviour during exploration phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLOIT_TRIALS = 1000\n",
    "\n",
    "population, metrics = agent.explore(env, EXPLOIT_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classifiers: 383\n",
      "Number of reliable classifiers: 0\n",
      "Percentage of successul trials: 1.90%\n",
      "\n",
      "Top 10 classifiers:\n",
      "##############X0-1-##############0X @ 0x10ee32160 \tq: 0.54 \tr: 0.51 \tir: 0.23 \texp: 17\n",
      "##############X0-3-##############0X @ 0x10edcfb70 \tq: 0.51 \tr: 0.52 \tir: 0.16 \texp: 8\n",
      "##############X0-2-##############0X @ 0x10ee9a9b0 \tq: 0.46 \tr: 0.52 \tir: 0.25 \texp: 20\n",
      "#0X#############-1-#X0############# @ 0x10ef2b208 \tq: 0.53 \tr: 0.45 \tir: 0.00 \texp: 3\n",
      "X0####0#########-3-0X############## @ 0x10ee9aac8 \tq: 0.66 \tr: 0.34 \tir: 0.00 \texp: 3\n",
      "0X##############-1-X0############## @ 0x10ee32b00 \tq: 0.47 \tr: 0.46 \tir: 0.00 \texp: 2\n",
      "##########0###X#-2-##########X###0# @ 0x10ee321d0 \tq: 0.43 \tr: 0.50 \tir: 0.21 \texp: 17\n",
      "#############0X#-1-#############X0# @ 0x10ed57f98 \tq: 0.44 \tr: 0.50 \tir: 0.23 \texp: 18\n",
      "X#####0#########-3-################ @ 0x10ef2bd30 \tq: 0.63 \tr: 0.34 \tir: 0.00 \texp: 9\n",
      "##X0##0#########-3-##0X############ @ 0x10ef2b8d0 \tq: 0.53 \tr: 0.40 \tir: 0.00 \texp: 3\n"
     ]
    }
   ],
   "source": [
    "population.sort(key=lambda cl: -cl.fitness)\n",
    "population_count = len(population)\n",
    "reliable_count = len([cl for cl in population if cl.is_reliable()])\n",
    "successful_trials = sum(m['environment']['found_reward'] for m in metrics)\n",
    "\n",
    "print(\"Number of classifiers: {}\".format(population_count))\n",
    "print(\"Number of reliable classifiers: {}\".format(reliable_count))\n",
    "print(\"Percentage of successul trials: {:.2f}%\".format(successful_trials / EXPLOIT_TRIALS  * 100))\n",
    "print(\"\\nTop 10 classifiers:\")\n",
    "for cl in population[:10]:\n",
    "    print(\"{!r} \\tq: {:.2f} \\tr: {:.2f} \\tir: {:.2f} \\texp: {}\".format(cl, cl.q, cl.r, cl.ir, cl.exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to reuse this knowledge - exploitation phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TRIALS = 100\n",
    "\n",
    "# Create new agent with initial classifiers population\n",
    "exploiter = ACS2(cfg, population=population)\n",
    "\n",
    "# Record performance, force erase previous run\n",
    "env = wrappers.Monitor(env, \"/tmp/gym-results\", force=True)\n",
    "\n",
    "_, metrics = exploiter.exploit(env, TEST_TRIALS)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of successul trials: 4.00%\n"
     ]
    }
   ],
   "source": [
    "successful_trials = sum(m['environment']['found_reward'] for m in metrics)\n",
    "\n",
    "print(\"Percentage of successul trials: {:.2f}%\".format(successful_trials / TEST_TRIALS * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
