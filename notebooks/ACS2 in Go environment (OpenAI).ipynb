{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym.envs.board_game import go\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACS2 in Go environment\n",
    "## Playing Go with Pachi\n",
    "OpenAI engine for playing Go uses *Pachi framework* (can be also used for other board games such as Weiqi or Baduk).\n",
    "\n",
    "From the [docs](http://repo.or.cz/pachi.git/blob_plain/HEAD:/README) we can read that: \n",
    "\n",
    "> The default engine plays by Chinese rules and should be about 7d KGS strength on 9x9. On 19x19 (using e.g. six-way Intel i7), it can hold a solid KGS 2d rank.  When using a large cluster (64 machines, 20 cores each), it maintains KGS 3d to 4d and has won e.g. a 7-stone handicap game against Zhou Junxun 9p. \n",
    "\n",
    "> By default, Pachi currently uses the UCT engine that combines Monte Carlo approach with tree search; UCB1AMAF tree policy using the RAVE method is used for tree search, while the Moggy playout policy using 3x3 patterns and various tactical checks is used for the semi-random Monte Carlo playouts.  Large-scale board patterns are used in the tree search.\n",
    "\n",
    "> At the same time, we keep trying a wide variety of other approaches and enhancements. Pachi is an active research platform and quite a few improvements have been already achieved. We rigorously play-test new features and enable them by default only when they give a universal strength boost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin with initializing the environment into initial state and visualizing state into console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Go9x9-v0')\n",
    "\n",
    "# Reset the state\n",
    "state = env.reset()\n",
    "\n",
    "# Render current state\n",
    "print(env._state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pachi uses it's own way of describing state of the Go play. To represent action we need to convert it from symbolic symbol (such as *A8*) to internal numeric representation. OpenAi delivers function for mapping but in order to know what actions are possible we need to know mapping between all representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moves_9x9():\n",
    "    \"\"\"\n",
    "    Return a list of all available moves (such as A8) on 9x9 board.\n",
    "    \"\"\"\n",
    "    rows = [chr(i) for i in range(ord('A'),ord('H')+1)]\n",
    "    cols = list(range(1, 8 + 1))\n",
    "    actions = []\n",
    "\n",
    "    for row in rows:\n",
    "        for col in cols:\n",
    "            actions.append(\"{}{}\".format(row, col))\n",
    "\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a function performing action mapping from numbers from `0-64` to Pachi internal representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_action(action_idx):\n",
    "    \"\"\"\n",
    "    Maps action with given index (0, num_actions) to Pachi representation.\n",
    "    \"\"\"\n",
    "    moves = [(move, go.str_to_action(env._state.board, move)) for move in moves_9x9()]\n",
    "    return moves[action_idx][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a map of all available actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_map = {i: map_action(i) for i in range(64)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a move let's pick up a random opening action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = random.choice(action_map)\n",
    "\n",
    "print(\"Random Pachi action id: [{}]\".format(action))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can execute it getting back all possible information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(action)\n",
    "\n",
    "print(\"Reward: [{}]\".format(reward))\n",
    "print(\"Done: [{}]\".format(done))\n",
    "print(info['state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyALCS code from local path\n",
    "import sys\n",
    "sys.path.append('/Users/khozzy/Projects/pyalcs')\n",
    "\n",
    "from alcs import ACS2, ACS2Configuration\n",
    "\n",
    "# Enable automatic module reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure agent. In this case we have to specify the classifier length (state of the board), number of possible actions (all possible moves). The state returned by the environment is not suitable to be used in ACS2 classifier. Therefore a special mapper function is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_state(state):\n",
    "    \"\"\"\n",
    "    Returns a flatten array of board state of given state.\n",
    "    Black stones are represented as 'B', whites as 'W'\n",
    "    \"\"\"\n",
    "    black_stones = state[0]\n",
    "    white_stones = state[1] * 2\n",
    "    \n",
    "    board = (black_stones + white_stones).astype('str')\n",
    "    board[board == '1'] = 'B'\n",
    "    board[board == '2'] = 'W'\n",
    "    \n",
    "    return list(board.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our agent to focus more on exploration than exploitation (`epsilon` parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFIER_LENGTH=env._state.board.size ** 2\n",
    "NUMBER_OF_POSSIBLE_ACTIONS=len(moves_9x9())\n",
    "\n",
    "cfg = ACS2Configuration(\n",
    "    classifier_length=CLASSIFIER_LENGTH,\n",
    "    number_of_possible_actions=NUMBER_OF_POSSIBLE_ACTIONS,\n",
    "    perception_mapper_fcn=map_state,\n",
    "    action_mapping_dict=action_map,\n",
    "    epsilon=0.7\n",
    ")\n",
    "\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build the agent using previously defined configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ACS2(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore best action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population, metrics = agent.explore(env, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
